{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a061b1a8-55bd-4a48-866f-21f6fed0a7a7",
   "metadata": {},
   "source": [
    "## Тестовое задание 2 от НРА\n",
    "### Копчев Владислав"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d403caf-cb67-4c09-9a0e-5d14d331034f",
   "metadata": {},
   "source": [
    "### Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559308dc-f548-4bf2-943c-f6e282861f5d",
   "metadata": {},
   "source": [
    "В данной работе я решу задачу автоматического разбиения текста на предложения. Среди задач информационного поиска хорошо изучена задача исправления ошибок в словах, что полезно при решении проблем, появляющихся при запросах из не более чем одного предложения. Однако при работе с базами документов, книг, статей очень часто возникает необходимость делать поисковые запросы, содержащие несколько последовательных предложений — и искать не все документы, которые содержат слова из этих предложений, а те документы, которые содержат именно эти предложения целиком (в Яндексе, например, для такого запроса необходимо заключить последовательность предложений в кавычки). Именно здесь возникает данная задача, применение алгоритмов машинного обучения к которой еще не настолько хорошо изучена, как в задачи исправления ошибок в словах. При выполнении задачи мною был разработан алгоритм, который на вход принимает путь до текстового файла с документом, а на выходе возвращает этот текст с корректным разбиением на предложения. \n",
    "\n",
    "У данного алгоритма есть несколько основных приложений. Во-первых, это облегчит работу редакторов — ведь редакторы тоже ошибаются, и $F$-мера вектора их разбиения текста на предложения при сопоставлении с вектором правильного разбиения может оказаться даже ниже, чем у алгоритма машинного обучения. Таким образом, алгоритм может помочь и менеджерам — ведь даст им обоснования для решения о сокращении штата редакторов, что позволит уменьшить расходы компании. Этим же алгоритм может помочь и экспертам-юристам — при чтении неграмотно набранного текста они могут запустить этот алгоритм и уже через минуту получить более пригодную для чтения версию этого текста. Также алгоритм может помочь пользователям информационного поиска — он поможет сделать жизнь пользователей, которым необходимо писать запросы из нескольких предложений, лучше, о чем говорилось в самом начале статьи. Соответственно, и разработчики поиска смогут понять, какой алгоритм лучше использовать для решения данной проблемы. Помочь алгоритм может и разработчикам нейросетей: проверить корректность табличных данных с числовыми признаками гораздо легче, чем корректность неструктурированного текста. Соответственно, этот алгоритм позволит улучшить данные, доступные NLP-специалистом, что позволит их алгоритмам работать лучше. И, наконец, алгоритм позволит решить ряд аналитических задач, которые невозможно решить без него: можно взять набор запросов, состоящих из нескольких предложений, пользователей системы КонсультантПлюс и понять, как выглядят статистические свойства ошибок польщователей, что позволит получить большое количество полезной информации о пользователях. Также можно посмотреть, в каком отделе редакторы плохо исправляют ошибки или из каких внешних источников в систему КонсультантПлюс поступают неграмотно набранные тексты, что позволит лучше оценивать эффективность сотрудников. Наконец, алгоритм имеет научную значимость, ведь показывают, что пунктуация текстов может иметь большое число интересных статистических закономерностей. \n",
    "\n",
    "Как следует из [статьи](https://www.dialog-21.ru/digests/dialog2008/materials/html/83.htm), на которую я буду опираться в ходе решения, при использовании простых эвристик точность доходит до 96%, а полнота — до 90%. Соответственно, алгоритм машинного обучения должен превзойти эти цифры,  а его $F$-мера должна быть не ниже $\\frac{2 \\cdot 0.96 \\cdot 0.9}{0.96+0.9}$ $\\approx$ 0.93. Если говорить про скорость работы, то при анализе документов ФАС было выяснено, что в них содержится не более 300 терминальных знаков препинания, а сами тексты не содержат очень большого числа символов. Соответственно, нет особого смысла при таких значениях длины входных данных описывать требования к асимптотическим оценкам работы алгоритма по времени в зависимости от размера входных данных. При таких входных данных гораздо большее значение будет иметь не асимптотическая сложность алгоритмов, которые использованы в решении, а то, насколько они хорошо оптимизированы их разработчиками. Соответственно, лучше всего поставить ко времени работы алгоритма следующее требование: алгоритм должен опираться на наиболее эффективно реализованные библиотеки для работы с алгоритмами машинного обучения, регулярными выражениями, неструктурированным текстом и т. д. Как будет видно в дальнейшем, всем этим требованиям разработанный агоритм соответствует. \n",
    "\n",
    "Алгоритм можно улучшать большим количеством способов. На данный момень алгоритм имеет важное ограничение: модель машинного обучения была обучена на текстах постановлений ФАС и не будет давать хороший результат, например, на художественных текстах. Чтобы исправить этот недостаток, можно добавить в обучающую выборку другие тексты. Также можно разработать новые признаки, которые повысят эффективность алгоритма. Можно добавить другие знаки пунктуации в качестве претендентов на конец предложения, что позволит исправлять больше ошибок в текстах. Можно использовать различные более совершенные модели машинного обучения.\n",
    "\n",
    "Ниже будет представлен описанный алгоритм и проведенные с ним эксперименты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e781eeb-d642-44a9-a206-96a79427d3fe",
   "metadata": {},
   "source": [
    "### Программа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02918b66-5c21-4134-b429-5c3aa5ae153d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Вкратце опишу принцип работы программы.\n",
    "\n",
    "Для начала вводится название текстового файла с кодировкой `cp1251` (поскольку именно такую кодировку имели файлы постановлений ФАС). После этого текст размечается. Для выполнения этой задачи была написана функция приблизительно как в эксперименте 1 из [статьи](https://www.dialog-21.ru/digests/dialog2008/materials/html/83.htm). Сначала формируем множества претендентов-предложений `possible_sentences` и претендентов-знаков `possible_ends_list`, записываем псевдослова в список `left_pseudowords_list`, `left_pseudowords_list`, после чего формируем таблицу признаков `possible_ends_df`. Далее идет загрузка предобученной модели (градиентный бустинг для классификации с подобранными с помощью `GridSearchCV` гиперпараметрами) и по предсказанным ей значениям восстановливается текст, в котором после каждого претендента-знака, который является концом предложения, вставляется символ перевода строки, а у следующего предложения убираются пробелы в начале — то есть, каждое предложение расположено на новой строке. Для работы программы необходим доступ к 3 файлам, содержащим заранее сформированные список сокращений, предобученную модель и предобученный OneHotEncoder.\n",
    "\n",
    "О терминологии:\n",
    "- Псевдослова — любая последовательность букв, цифр или символов (за исключением `\\n`, `\\t`, букв не из английского или русского алфавита и некоторых других символов) до пробела или символа перевода строки\n",
    "- Претенденты-предложения — любая последовательность псевдослов до терминального знака (`!`, `.`, `?`).\n",
    "- Претенденты-знаки — терминальные знаки\n",
    "\n",
    "О признаках:\n",
    "- `sign`: терминальный знак\n",
    "- `sign index`: его индекс в тексте\n",
    "- `right`: ближайшее псевдослово справа (в нормальной форме)\n",
    "- `left`: ближайшее псевдослово слева\n",
    "- `dleft`: количество псевдослов слева\n",
    "- `dright`: количество пседовлов справа\n",
    "- `abbleft`: наличие ближайшего псевдослова слева в списке сокращений\n",
    "- `abbright`: наличие ближайшего псевдослова справа в списке сокращений\n",
    "- `cright`: список характеристик правого ближайшего псевдослова (Кириллица, Латиница, Числа, Большая/маленькая заглавная буква)\n",
    "- `cleft`: список характеристик левого ближайшего псевдослова (Кириллица, Латиница, Числа, Большая/маленькая заглавная буква)\n",
    "- `wright`: ближайшее псевдослово справа, содержащее не меньше 1 буквы или цифры\n",
    "- `cwright` количество таких псевдослов справа\n",
    "- `end?`: целевая переменная, 1 — конец предложения, 0 — нет\n",
    "\n",
    "Для составления списка сокращений воспользуемся корпусом OpenCorpora.\n",
    "\n",
    "Также я добавил собственные признаки:\n",
    "\n",
    "- `left_caps`: набрано ли левое псевдослово только заглавными буквами?\n",
    "- `right_caps`: аналогично про правое"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e817fd0-80cf-4878-825a-52dc95552f6b",
   "metadata": {},
   "source": [
    "Итак, чтобы запустить программу, необходимо запустить следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b8502ca-afde-457b-a34b-c7c93d4ad5dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите название текстового файла с постановлением ФАС Уральского округа\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " raw_text/Постановление ФАС Уральского округа от 05_06_2006 N Ф09-4478.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "г.Ф09-4478/06-С3\n",
      "\n",
      "Федеральный арбитражный суд Уральского округа в составе председательствующего Гайдука А.А., судей Черкасской Г.Н., Купреенкова В.А.рассмотрел в судебном заседании кассационную жалобу открытого акционерного общества \"Уралсвязьинформ\" (далее - общество) на решение суда первой инстанции от 27.01.2006 (резолютивная часть от 24.01.2006) и постановление суда апелляционной инстанции от 30.03.2006 Арбитражного суда Свердловской области по делу N А60-34123/2005-С3.\n",
      "\n",
      "В судебном заседании принял участие представитель общества - Соломеин Е.А.доверенность от 22.12.2005).\n",
      "\n",
      "Представитель государственного унитарного предприятия \"Уралэнергочермет-ДС\" (далее - предприятие), надлежащим образом извещенного о времени и месте судебного разбирательства, в судебное заседание не явился.\n",
      "\n",
      "\n",
      "Предприятие обратилось в Арбитражный суд Свердловской области с иском к обществу об обязании выплатить компенсацию за долю в общей долевой собственности на не завершенный строительством II производственный комплекс автоматической телефонной станции в пос.\n",
      "Рудничный в размере 47,37% от общей собственности в сумме 191278 руб.00 коп.с переходом права собственности на эту долю к ответчику.\n",
      "\n",
      "Решением суда первой инстанции от 27.01.2006 (резолютивная часть от 24.01.2006; судья Родичко Н.В.исковые требования удовлетворены.\n",
      "\n",
      "Постановлением суда апелляционной инстанции от 30.03.2006 (судьи Лиходумова С.Н., Анисимов Л.А., Цветкова С.А.решение оставлено без изменения.\n",
      "\n",
      "В жалобе, поданной в Федеральный арбитражный суд Уральского округа, предприятие просит решение суда первой инстанции и постановление суда апелляционной инстанции отменить, в удовлетворении исковых требований отказать, ссылаясь на неправильно применение судом норм, содержащихся в ст.131 Гражданского кодекса Российской Федерации, ст.8 Федерального закона от 07.07.2003 N 126-ФЗ \"О связи\", п.5 Положения \"Об особенностях регистрации права собственности и других вещных прав на линейно-кабельные сооружения\", утвержденного Постановлением Правительства Российской Федерации от 11.02.2005 N 68.\n",
      "\n",
      "\n",
      "Законность обжалуемых судебных актов проверена судом кассационной инстанции в порядке, предусмотренном ст.274, 284, 286 Арбитражного процессуального кодекса Российской Федерации, на основании доводов, изложенных в кассационной жалобе.\n",
      "\n",
      "Как следует из материалов дела, не завершенный строительством II производственный комплекс автоматической телефонной станции в пос.\n",
      "Рудничный принадлежит на праве долевой собственности ответчику (52,63%) и Свердловской области (47,37%).\n",
      "На основании постановления Правительства Свердловской области от 02.09.2003 N 544-пп Свердловская область передала долю в праве общей долевой собственности на вышеуказанный объект связи в хозяйственное ведение предприятия (акт приема-передачи от 25.09.2003).\n",
      "\n",
      "Ссылаясь на невозможность достижения соглашения о распоряжении общим имуществом и использовании его совместно для получения доходов, истец обратился в суд с требованием об обязании ответчика выплатить стоимость его доли.\n",
      "\n",
      "В соответствии с правилами, установленными ст.246 - 248 Гражданского кодекса Российской Федерации, распоряжение имуществом, находящимся в общей долевой собственности, а также получение доходов от его использования осуществляется по соглашению всех участников общей долевой собственности.\n",
      "\n",
      "При недостижении участниками долевой собственности соглашения о способе и условиях раздела общего имущества или выдела доли одного из них участник долевой собственности вправе в судебном порядке требовать выдела в натуре своей доли из общего имущества.\n",
      "Если выдел доли в натуре не допускается законом или невозможен без несоразмерного ущерба имуществу, находящемуся в общей собственности, выделяющийся собственник имеет право на выплату ему стоимости его доли другими участниками долевой собственности (п.3 ст.252 Гражданского кодекса Российской Федерации).\n",
      "\n",
      "Установив, что истец неоднократно предлагал ответчику определить порядок владения и пользования АТС, с целью выкупа своей доли направлял в адрес ответчика проекты договоров купли-продажи от 09.12.2003 N 76-8, от 11.02.2004 N 76-8, от 28.03.2005 N 38, однако предложения истца были оставлены ответчиком без внимания, суд первой инстанции обоснованно удовлетворил заявленные требования.\n",
      "Суд апелляционной инстанции, руководствуясь положениями п.3 ст.252 Гражданского кодекса Российской Федерации, признал заявленные требования обоснованными, размер компенсации - подтвержденным материалами дела, и оставил решение без изменения.\n",
      "\n",
      "При этом суды первой и апелляционной инстанций правомерно отклонили доводы ответчика о том, что исковые требования не подлежат удовлетворению, поскольку у истца отсутствует зарегистрированное право хозяйственного ведения на долю в не завершенном строительством II производственном комплексе автоматической телефонной станции в пос.Рудничный.\n",
      "Учитывая, что истец не представил надлежащих доказательств правомерности отнесения указанного производственного комплекса автоматической телефонной станции к объектам недвижимого имущества в соответствии с нормами, содержащимися в ст.130 Гражданского кодекса Российской Федерации, ст.8 Федерального закона от 07.07.2003 N 126-ФЗ \"О связи\", п.3 Положения \"Об особенностях регистрации права собственности и других вещных прав на линейно-кабельные сооружения\", утвержденного Постановлением Правительства Российской Федерации от 11.02.2005 N 68, суды пришли к правильному выводу об отсутствии необходимости обязательной государственной регистрации прав на данный объект.\n",
      "\n",
      "Доводы заявителя кассационной жалобы подлежат отклонению.\n",
      "По мнению общества, копии справок формы КС-2 и КС-3 подтверждают факт наличия в составе спорного объекта кабельной канализации, являющейся недвижимым имуществом.\n",
      "Между тем, данные документы не являются допустимым доказательством того, что не завершенный строительством II производственный комплекс автоматической телефонной станции в пос.\n",
      "Рудничный является недвижимым имуществом.\n",
      "\n",
      "Нарушений при рассмотрении дела судами первой и апелляционной инстанций норм материального и процессуального права, которые в соответствии со ст.288 Арбитражного процессуального кодекса Российской Федерации являются основаниями к отмене или изменению судебных актов, не установлено.\n",
      "\n",
      "Таким образом, решение суда первой инстанции и постановление суда апелляционной инстанции являются законными и обоснованными, кассационная жалоба удовлетворению не подлежит.\n",
      "\n",
      "Руководствуясь ст.286, 289 Арбитражного процессуального кодекса Российской Федерации, суд\n",
      "\n",
      "ПОСТАНОВИЛ:\n",
      "\n",
      "решение суда первой инстанции от 27.01.2006 (резолютивная часть от 24.01.2006) и постановление суда апелляционной инстанции от 30.03.2006 Арбитражного суда Свердловской области по делу N А60-34123/2005-С3 оставить без изменения, кассационную жалобу открытого акционерного общества \"Уралсвязьинформ\" - без удовлетворения.\n",
      "\n",
      "\n",
      "Председательствующий\n",
      "ГАЙДУК А.А.\n",
      "\n",
      "\n",
      "Судьи\n",
      "ЧЕРКАССКАЯ Г.Н.\n",
      "\n",
      "КУПРЕЕНКОВ В.А.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd87d0-47a1-4a54-b456-135cb41a06ed",
   "metadata": {},
   "source": [
    "### Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b5fae-226c-46cb-8c6c-7b874ad0728d",
   "metadata": {},
   "source": [
    "Для тестирования работы алгоритма я разметил 20 текстов постановлений ФАС (для удобства разметки была написана отдельная функция) и для каждого размеченного документа сформировал тестовую выборку, состояющую из него самого, и обучающую из остальных 19 — ведь именно так алгоритм и будет использоваться, он будет обучен на всех размеченных текстах постановлений ФАС и будет принимать какой-то один другой текст на вход. Посмотрим на $F$-меру при использовании бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5721e10-c898-4ec0-864e-cf7233252a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.9655172413793104\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9599999999999999\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9859154929577464\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab62449-a0c5-449a-bcd5-e8433e1f7ae8",
   "metadata": {},
   "source": [
    "Мы видим, что $F$-мера $\\geq 0.95$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7e09b-9a99-4ec7-a79d-c956221301da",
   "metadata": {},
   "source": [
    "Протестируем таким же способом другие алгоритмы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ec3a020-617a-42e1-a6be-2a7e0dd09882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.983050847457627\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9599999999999999\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9859154929577464\n",
      "LogisticRegression\n",
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.9655172413793104\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9736842105263158\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9714285714285714\n",
      "RandomForestClassifier\n",
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.9655172413793104\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9599999999999999\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9859154929577464\n",
      "GradientBoostingClassifier\n",
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.9655172413793104\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9736842105263158\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9859154929577464\n",
      "GradientBoostingClassifier\n",
      "1 / 20 0.961038961038961\n",
      "2 / 20 1.0\n",
      "3 / 20 0.9824561403508771\n",
      "4 / 20 0.9824561403508771\n",
      "5 / 20 1.0\n",
      "6 / 20 0.9655172413793104\n",
      "7 / 20 1.0\n",
      "8 / 20 0.975609756097561\n",
      "9 / 20 0.9866666666666666\n",
      "10 / 20 1.0\n",
      "11 / 20 0.9841269841269841\n",
      "12 / 20 1.0\n",
      "13 / 20 1.0\n",
      "14 / 20 0.988235294117647\n",
      "15 / 20 0.9876543209876543\n",
      "16 / 20 1.0\n",
      "17 / 20 0.9736842105263158\n",
      "18 / 20 0.9555555555555556\n",
      "19 / 20 0.9803921568627451\n",
      "20 / 20 0.9859154929577464\n"
     ]
    }
   ],
   "source": [
    "test_other()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25218a-2ffd-4a35-a277-6ad3d88d1e66",
   "metadata": {},
   "source": [
    "Итак, мы видим, что все алгоритмы со стандартными значениями гиперпараметров дают примерно одинаковое качество работы. Также в результате тюнинга гиперпараметров для SVC был сделан вывод, что SVC дает наилучший результат при `clf = svm.SVC(C=0.24, kernel='linear')`.\n",
    "\n",
    "Помимо этого, был проведен еще один эксперимент, в ходе которого протестирован признак `left_length` — длина левого псевдослова. Он показал себя плохо, ухудшив $F$-меру для каждого алгоритма, что ожидаемо — вряд ли тут есть какая-то значимая корреляция с таргетом.\n",
    "\n",
    "И, наконец, был проведен эксперимент, в ходе которого были размечены 1–12 примеры из статьи, чтобы проверить работу алгоритма вне документов. Результаты были удовлетворительные, однако при обучении бустинга было решено не включать эти документы в обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe59b45-12dd-452f-a016-64698c274327",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326f178-fcc7-435d-ba10-c835c08c4fc8",
   "metadata": {},
   "source": [
    "Итак, был разработан алгоритм автоматического разбиения текста на предложения, удовлетворяющий поставленным требованиям по скорости и эффективности работы. Были проведены несколько экспериментов, описаны приложения алгоритма. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937b047-8690-451e-9e5c-a5c82cfeae83",
   "metadata": {},
   "source": [
    "### Приложение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ef09c-29ab-4417-ade7-86d1c0439cba",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Функция для составления списка сокращений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ff8e3-b838-4a34-b617-89a443ac57e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_abrs():\n",
    "    # !wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip; open annot.opcorpora.xml.zip\n",
    "    corpus = opencorpora.load('annot.opcorpora.xml')\n",
    "    abr = corpus.sentences \n",
    "    abbreviatures = []\n",
    "\n",
    "    for sent in abr:\n",
    "        srch = re.findall('(^|\\s)' + '[А-я]*' + '[.]+[ ]+[А-Я]+[А-я]*', sent.source)\n",
    "        if srch:\n",
    "            abbreviatures.append(sent.source)\n",
    "\n",
    "    with open(\"abbrevs\", \"wb\") as fp:\n",
    "        pickle.dump(abbreviatures, fp)\n",
    "    fp.close()\n",
    "        \n",
    "# save_abrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cc6fc-480b-4a47-ba2c-dba82cef3c80",
   "metadata": {},
   "source": [
    "2. Функция для разметки текстов ФАС:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e931356-e376-459c-bb7f-07edd55f02b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def markup():\n",
    "    import re\n",
    "    import pickle\n",
    "\n",
    "    filename = input()\n",
    "\n",
    "    with open('raw_text/' + filename + '.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        # считываем сразу весь файл; урааааа\n",
    "        text = fp.read()\n",
    "        fp.close()\n",
    "\n",
    "    # text = 'Но ведь Маша знает А. Б. Иванова много лет и никогда про него ничего плохого  не слышала!!'\n",
    "\n",
    "    possible_sentences = re.findall('[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*[.!?]', text)\n",
    "\n",
    "    pattern = '[.!?]'\n",
    "    possible_ends = []\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    ans = []\n",
    "\n",
    "    for m in p.finditer(text):\n",
    "        index = m.start()\n",
    "        character = m.group()\n",
    "        possible_ends.append(character)\n",
    "\n",
    "        # print(possible_sentences)\n",
    "\n",
    "    for i in range(len(possible_sentences)):\n",
    "        cnt = 1\n",
    "        if i > 0:\n",
    "            print(possible_sentences[i-1], end='')\n",
    "            cnt += 1\n",
    "        print(possible_sentences[i], end='')\n",
    "        if i < len(possible_sentences) - 1:\n",
    "            print(possible_sentences[i+1])\n",
    "        else:\n",
    "            print()\n",
    "        print(str(i+1) + '/' + str(len(possible_sentences)), sep='')\n",
    "        print(str(cnt) + '-й по счету в данной строке ' + 'знак ' + possible_ends[i] + ' является концом или нет? (0/1)')\n",
    "        cur_ans = int(input())\n",
    "        ans.append(cur_ans)\n",
    "        print()\n",
    "\n",
    "    print(ans)\n",
    "\n",
    "    with open('raw_text/' + filename + '_rzm', \"wb\") as fp:\n",
    "        pickle.dump(ans, fp)\n",
    "        fp.close()\n",
    "\n",
    "    # save_abrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742658e-2822-4bf3-9f0a-11b4b9da2e87",
   "metadata": {},
   "source": [
    "3. Функция для тестирования модели с бустингом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f6f602c-2c8d-475c-a275-8046f1623566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def find_abb(word, abbreviatures_list):\n",
    "    abbreviatures = dict()\n",
    "    for sent in abbreviatures_list:\n",
    "        srch = re.findall('(^|\\s)' + word + '[.]+[ ]+[А-Я]+[А-я]*', sent)\n",
    "        if srch:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def make_table(text, answers):\n",
    "    possible_sentences = re.findall('[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*[.!?]', text)\n",
    "\n",
    "    pattern = '[.!?]'\n",
    "    possible_ends_list = []\n",
    "    possible_ends_indices = []\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    for m in p.finditer(text):\n",
    "        index = m.start()\n",
    "        character = m.group()\n",
    "        possible_ends_list.append(character)\n",
    "        possible_ends_indices.append(index)\n",
    "\n",
    "    left_pseudowords_list = []\n",
    "    right_pseudowords_list = []\n",
    "    left_pseudowords_count = []\n",
    "    right_pseudowords_count = []\n",
    "    right_abb_list = []\n",
    "    left_abb_list = []\n",
    "    cright_list = []\n",
    "    cleft_list = []\n",
    "    wright_list = []\n",
    "    cwright_list = []\n",
    "    left_lengths = []\n",
    "    left_caps = []\n",
    "    right_caps = []\n",
    "\n",
    "    for i in range(len(possible_sentences)):\n",
    "        # load data\n",
    "        cur_sentence = possible_sentences[i]\n",
    "        if i + 1 != len(possible_sentences):\n",
    "            next_sentence = possible_sentences[i + 1]\n",
    "        else:\n",
    "            next_sentence = ''\n",
    "            \n",
    "        # clean data\n",
    "        cur_sentence = re.sub(' +', ' ', cur_sentence).strip('( )*')    \n",
    "        next_sentence = re.sub(' +', ' ', next_sentence).strip('( )*')\n",
    "\n",
    "        # extracting pseudowords from data\n",
    "        left_pseudowords = re.split('[(\\n)|( )]', cur_sentence)\n",
    "        right_pseudowords = re.split('[(\\n)|( )]', next_sentence)\n",
    "        left_pseudoword = left_pseudowords[-1]\n",
    "        right_pseudoword = right_pseudowords[0]\n",
    "        left_abb = 0\n",
    "        right_abb = 0\n",
    "        cleft = ''\n",
    "        cright = ''\n",
    "        wright = list(filter(lambda v: match('[ЁёА-я0-9A-Za-z]+[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*', v), \n",
    "                                  right_pseudowords))        \n",
    "        cwright = ''\n",
    "\n",
    "        if len(wright) == 0:\n",
    "            wright = '---'\n",
    "        else:\n",
    "            wright = wright[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "\n",
    "            \"\"\"\n",
    "            testing\n",
    "            print(cur_sentence, left_pseudowords, left_pseudoword, len(left_pseudowords))\n",
    "            print(next_sentence, right_pseudowords, right_pseudoword, len(right_pseudowords))\n",
    "            print(len(possible_ends_list) == len(possible_sentences))\n",
    "            \"\"\"\n",
    "\n",
    "        if len(left_pseudoword) > 1:\n",
    "            left_pseudoword = left_pseudoword.strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "            left_abb = find_abb(left_pseudoword, abr)\n",
    "        elif left_pseudowords == ['']:\n",
    "            left_pseudowords = []\n",
    "        if len(left_pseudoword) == 0:\n",
    "            left_pseudowords = []\n",
    "            left_pseudoword = '---'\n",
    "\n",
    "        if len(right_pseudoword) > 1:\n",
    "            right_pseudoword = right_pseudowords[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "            right_abb = find_abb(right_pseudoword, abr)\n",
    "        elif right_pseudowords == ['']:\n",
    "            right_pseudowords = []\n",
    "        if len(right_pseudoword) == 0:\n",
    "            right_pseudowords = []\n",
    "            right_pseudoword = '---'\n",
    "\n",
    "        if wright != '---':\n",
    "            is_cyrillic = re.findall('[А-я]', wright)\n",
    "            is_latin = re.findall('[A-z]', wright)\n",
    "            is_numeric = re.findall('[0-9]', wright)\n",
    "            is_uppercase = re.findall('[А-Я]', wright[0])\n",
    "            is_lowercase = re.findall('[а-я]', wright[0])\n",
    "            is_first_num = re.findall('[0-9]', wright[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', wright[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cwright += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Punctuation first'\n",
    "\n",
    "            if len(cwright) == 0:\n",
    "                cwright = '---'\n",
    "        else:\n",
    "            cwright = '---'\n",
    "\n",
    "        if len(right_pseudoword) > 0:\n",
    "            is_cyrillic = re.findall('[А-я]', right_pseudoword)\n",
    "            is_latin = re.findall('[A-z]', right_pseudoword)\n",
    "            is_numeric = re.findall('[0-9]', right_pseudoword)\n",
    "            is_uppercase = re.findall('[А-Я]', right_pseudoword[0])\n",
    "            is_lowercase = re.findall('[а-я]', right_pseudoword[0])\n",
    "            is_first_num = re.findall('[0-9]', right_pseudoword[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', right_pseudoword[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cright += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Punctuation first'\n",
    "\n",
    "            if len(cright) == 0:\n",
    "                cright = '---'\n",
    "\n",
    "        else:\n",
    "            cright = '---'\n",
    "\n",
    "        if len(left_pseudoword) > 0:\n",
    "            is_cyrillic = re.findall('[А-я]', left_pseudoword)\n",
    "            is_latin = re.findall('[A-z]', left_pseudoword)\n",
    "            is_numeric = re.findall('[0-9]', left_pseudoword)\n",
    "            is_uppercase = re.findall('[А-Я]', left_pseudoword[0])\n",
    "            is_lowercase = re.findall('[а-я]', left_pseudoword[0])\n",
    "            is_first_num = re.findall('[0-9]', left_pseudoword[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', left_pseudoword[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cleft += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Punctuation first'\n",
    "\n",
    "            if len(cleft) == 0:\n",
    "                cleft = '---'\n",
    "\n",
    "        else:\n",
    "            cleft = '---'\n",
    "\n",
    "            # saving pseudowords extracted data\n",
    "        left_pseudowords_list.append(morph.parse(left_pseudoword)[0].normal_form)\n",
    "        right_pseudowords_list.append(morph.parse(right_pseudoword)[0].normal_form)\n",
    "        left_pseudowords_count.append(len(left_pseudowords))\n",
    "        right_pseudowords_count.append(len(right_pseudowords))\n",
    "        left_abb_list.append(left_abb)\n",
    "        right_abb_list.append(right_abb)\n",
    "        cleft_list.append(cleft)\n",
    "        cright_list.append(cright)\n",
    "        wright_list.append(morph.parse(wright)[0].normal_form)\n",
    "        cwright_list.append(cwright)\n",
    "        # my\n",
    "        left_lengths.append(len(left_pseudoword))\n",
    "        left_caps.append(len(re.findall('[А-Я]', left_pseudoword)) == len(left_pseudoword))\n",
    "        right_caps.append(len(re.findall('[А-Я]', right_pseudoword)) == len(right_pseudoword))\n",
    "\n",
    "        data_for_df = dict()\n",
    "\n",
    "        data_for_df['sign'] = possible_ends_list\n",
    "        data_for_df['sign index'] = possible_ends_indices\n",
    "        data_for_df['right'] = right_pseudowords_list\n",
    "        data_for_df['left'] = left_pseudowords_list\n",
    "        data_for_df['dleft'] = left_pseudowords_count\n",
    "        data_for_df['dright'] = right_pseudowords_count\n",
    "        data_for_df['abbleft'] = left_abb_list\n",
    "        data_for_df['abbright'] = right_abb_list\n",
    "        data_for_df['cright'] = cright_list\n",
    "        data_for_df['cleft'] = cleft_list\n",
    "        data_for_df['wright'] = wright_list\n",
    "        data_for_df['cwright'] = cwright_list\n",
    "        data_for_df['left_caps'] = left_caps\n",
    "        data_for_df['right_caps'] = right_caps\n",
    "        data_for_df['end?'] = answers\n",
    "\n",
    "    possible_ends_df = pd.DataFrame(data_for_df)\n",
    "    \n",
    "    return possible_ends_df\n",
    "\n",
    "def test():\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-3415_rzm', \"rb\") as fp:\n",
    "        anss1 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-3415.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text1 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee1 = make_table(text1, anss1)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-2600_rzm', \"rb\") as fp:\n",
    "        anss2 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-2600.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text2 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee2 = make_table(text2, anss2)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4528_rzm', \"rb\") as fp:\n",
    "        anss3 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4528.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text3 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee3 = make_table(text3, anss3)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1066_rzm', \"rb\") as fp:\n",
    "        anss4 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1066.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text4 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee4 = make_table(text4, anss4)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4409_rzm', \"rb\") as fp:\n",
    "        anss5 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4409.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text5 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee5 = make_table(text5, anss5)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 15_12_2011 N Ф09-8320_rzm', \"rb\") as fp:\n",
    "        anss6 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 15_12_2011 N Ф09-8320.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text6 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee6 = make_table(text6, anss6)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4438_rzm', \"rb\") as fp:\n",
    "        anss7 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4438.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text7 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee7 = make_table(text7, anss7)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 29_01_2002 N Ф09-92 0_rzm', \"rb\") as fp:\n",
    "        anss8 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 29_01_2002 N Ф09-92 0.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text8 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee8 = make_table(text8, anss8)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 27_07_2012 N Ф09-6215_rzm', \"rb\") as fp:\n",
    "        anss9 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 27_07_2012 N Ф09-6215.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text9 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee9 = make_table(text9, anss9)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4435_rzm', \"rb\") as fp:\n",
    "        anss10 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4435.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text10 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee10 = make_table(text10, anss10); плохо сделал именно этот текст\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4559_rzm', \"rb\") as fp:\n",
    "        anss11 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4559.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text11 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee11\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-9906_rzm', \"rb\") as fp:\n",
    "        anss12 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-9906.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text12 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee12\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4336_rzm', \"rb\") as fp:\n",
    "        anss13 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4336.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text13 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee13\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 05_06_2006 N Ф09-205_rzm', \"rb\") as fp:\n",
    "        anss14 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 05_06_2006 N Ф09-205.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text14 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee14\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4131_rzm', \"rb\") as fp:\n",
    "        anss15 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4131.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text15 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee15\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 06_02_2007 N Ф09-4881_rzm', \"rb\") as fp:\n",
    "        anss16 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 06_02_2007 N Ф09-4881.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text16 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee16\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_07_2012 N Ф09-1027_rzm', \"rb\") as fp:\n",
    "        anss17 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_07_2012 N Ф09-1027.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text17 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee17\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4469_rzm', \"rb\") as fp:\n",
    "        anss18 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4469.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text18 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee18\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1060_rzm', \"rb\") as fp:\n",
    "        anss19 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1060.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text19 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee19\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4398_rzm', \"rb\") as fp:\n",
    "        anss20 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4398.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text20 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee20\n",
    "\n",
    "    def ohe_tr(onehotencoder, data, categorical_cols, not_fit):\n",
    "        if not_fit:\n",
    "            transformed_data = onehotencoder.fit_transform(data[categorical_cols])\n",
    "        else:\n",
    "            transformed_data = onehotencoder.transform(data[categorical_cols])\n",
    "        encoded_data = pd.DataFrame(transformed_data, index=data.index)\n",
    "        other_data = data.drop(columns=categorical_cols)\n",
    "        data_out = pd.concat([encoded_data, other_data], axis=1)\n",
    "        return data_out\n",
    "\n",
    "    def train_test(n):\n",
    "        texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10,\n",
    "                text11, text12, text13, text14, text15, text16, text17, text18, text19,\n",
    "                text20]\n",
    "        anss = [anss1, anss2, anss3, anss4, anss5, anss6, anss7, anss8, anss9, anss10,\n",
    "               anss11, anss12, anss13, anss14, anss15, anss16, anss17, anss18, anss19,\n",
    "               anss20]\n",
    "\n",
    "        bigtext = ''\n",
    "        bigans = []\n",
    "\n",
    "        for text in texts[:n] + texts[n + 1:]:\n",
    "            bigtext += text\n",
    "\n",
    "        for ans in anss[:n] + anss[n + 1:]:\n",
    "            bigans += ans\n",
    "\n",
    "        table_train = make_table(bigtext, bigans)\n",
    "        table_test = make_table(texts[n], anss[n])\n",
    "\n",
    "        X_train = table_train.drop(['sign index', 'end?'], axis=1)\n",
    "        y_train = table_train['end?']\n",
    "\n",
    "        categorical = ['sign', 'wright', 'right', 'left', 'abbleft', 'abbright', 'cright', 'cleft', 'cwright', 'left_caps', 'right_caps']\n",
    "\n",
    "        X_train_trans = ohe_tr(enc, X_train, categorical, True)\n",
    "\n",
    "        clf = GradientBoostingClassifier(loss='deviance', max_depth=3)\n",
    "        clf.fit(X_train_trans, y_train)\n",
    "\n",
    "        X_test = table_test.drop(['sign index', 'end?'], axis=1)\n",
    "        y_test = table_test['end?']\n",
    "\n",
    "        X_test_trans = ohe_tr(enc, X_test, categorical, False)\n",
    "        y_pred = clf.predict(X_test_trans)\n",
    "        \n",
    "        return f1_score(y_test, y_pred)\n",
    "\n",
    "    cnt = 20\n",
    "    for k in range(0, cnt):\n",
    "        print(k + 1, '/', cnt, train_test(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd19ef-1ae1-42c6-85ec-182432a160a6",
   "metadata": {},
   "source": [
    "4. Функция для `fit` OneHotEncoder'а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed34d6-e317-4150-ba1d-25bc99df1eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10,\n",
    "        text11, text12, text13, text14, text15, text16, text17, text18, text19,\n",
    "        text20]\n",
    "anss = [anss1, anss2, anss3, anss4, anss5, anss6, anss7, anss8, anss9, anss10,\n",
    "        anss11, anss12, anss13, anss14, anss15, anss16, anss17, anss18, anss19,\n",
    "        anss20]\n",
    "\n",
    "bigtext = ''\n",
    "bigans = []\n",
    "\n",
    "for text in texts:\n",
    "    bigtext += text\n",
    "\n",
    "for ans in anss:\n",
    "    bigans += ans\n",
    "\n",
    "table_train = make_table(bigtext)\n",
    "X_train = table_train.drop(['sign index'], axis=1)\n",
    "categorical = ['sign', 'wright', 'right', 'left', 'abbleft', 'abbright', 'cright', 'cleft', 'cwright', 'left_caps', 'right_caps']\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n",
    "enc.fit(X_train[categorical])\n",
    "\n",
    "with open('enc.pkl','wb') as f:\n",
    "    pickle.dump(enc,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36417c43-5734-457b-a4f4-58f9043cf01a",
   "metadata": {},
   "source": [
    "5. Функция для обучения бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c6626-eb86-4bd5-b0e9-90acc52e8a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10,\n",
    "        text11, text12, text13, text14, text15, text16, text17, text18, text19,\n",
    "        text20]\n",
    "anss = [anss1, anss2, anss3, anss4, anss5, anss6, anss7, anss8, anss9, anss10,\n",
    "        anss11, anss12, anss13, anss14, anss15, anss16, anss17, anss18, anss19,\n",
    "        anss20]\n",
    "\n",
    "bigtext = ''\n",
    "bigans = []\n",
    "\n",
    "for text in texts:\n",
    "    bigtext += text\n",
    "\n",
    "for ans in anss:\n",
    "    bigans += ans\n",
    "\n",
    "table_train = make_table(bigtext, bigans)\n",
    "table_test = make_table(text1, anss1)\n",
    "\n",
    "X_train = table_train.drop(['sign index', 'end?'], axis=1)\n",
    "y_train = table_train['end?']\n",
    "    \n",
    "categorical = ['sign', 'wright', 'right', 'left', 'abbleft', 'abbright', 'cright', 'cleft', 'cwright', 'left_caps', 'right_caps']\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n",
    "X_train_trans = ohe_tr(enc, X_train, categorical, True)\n",
    "\n",
    "clf = GradientBoostingClassifier(loss='deviance', max_depth=3)\n",
    "clf.fit(X_train_trans, y_train)\n",
    "\n",
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(clf,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71904f8e-5e3f-4876-b893-04e733b0168b",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. Функция для получения признаков для примеров 1–12 из статьи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c4f30-724e-452e-94ec-a2c4ba68bbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = ['Конечно, в рамках газетной статьи невозможно сделать обзор сотни докладов, поэтому мы рекомендуем посетить Интернет-страницу конференции http://www.ict.nsc.ru/ws/mol2000/, на которой размещены программа мероприятия и тезисы докладов.',\n",
    "        'В 11.45 дали слово Кудрину , но он всё не шёл.',\n",
    "        'выполнение 12 тепловозам усиленного ТР-1 с применением средств диагностики вместо ТР-2 дало экономический эффект 221,8 тыс. руб.',\n",
    "        \"\"\"Параметры системы питания :\n",
    "        линейное напряжение на проводах 81 ... 83 , В ......... 220\n",
    "        фазное напряжение на проводах 81 ... 83 по отношению\n",
    "        к проводу 84 ( нулю ), В ................................................ 127\n",
    "        частота переменного тока, Гц .................................... 50\n",
    "        выпрямленное напряжение на проводах, В\n",
    "        15--30............................................................... 110\n",
    "        44--30................................................................. 50\"\"\",\n",
    "        'Режиссёр Михаил. Бычков поставил в Таллине притчу о любви к невозможному и о презрении к реальности .',\n",
    "        'Дело в том, что по всяким планам «пятилеток» и заданиям ЦК советский военный комплекс создавал ядерное оружие с запасом на пять и более(!) ядерных войн.',\n",
    "        'Пролетели \"Тише!\" Виктора Косаковского и \"Фрески\" Александра Гутмана.',\n",
    "        'Не в лесе и не в медицине дело...',\n",
    "        'Только вот ради чего ?!',\n",
    "        'В связи с этим первый интервал пробегов был принят равным 350...700 тыс. км (середина интервала - 525 тыс. км), второй интервал -- 700...1050 тыс. км (середина интервала - 875 тыс. км) и третий интервал 1050...1400 тыс. км (середина интервала -- 1225 тыс. км).',\n",
    "        'В случае нарушений выносится письменное предупреждение: «В вашей деятельности допускаются вот такие недочёты ...»',\n",
    "        ]\n",
    "answers = [[0, 0, 0, 1],\n",
    "          [0, 1],\n",
    "          [0, 1],\n",
    "          [0] * (\"\"\"Параметры системы питания :\n",
    "                линейное напряжение на проводах 81 ... 83 , В ......... 220\n",
    "                фазное напряжение на проводах 81 ... 83 по отношению\n",
    "                к проводу 84 ( нулю ), В ................................................ 127\n",
    "                частота переменного тока, Гц .................................... 50\n",
    "                выпрямленное напряжение на проводах, В\n",
    "                15--30............................................................... 110\n",
    "                44--30................................................................. 50\"\"\".count('.') - 1) + [1],\n",
    "          [0, 1],\n",
    "          [0, 1],\n",
    "          [0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1],\n",
    "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "trytexts = texts[0]\n",
    "tryanswers = answers[0]\n",
    "\n",
    "for trytext in texts[1:]:\n",
    "    trytexts += '\\n'\n",
    "    trytexts += trytext\n",
    "    \n",
    "for tryanswer in answers[1:]:\n",
    "    tryanswers += tryanswer\n",
    "    \n",
    "table = make_table(trytexts, tryanswers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed2a8f-1fd3-408f-a4bc-65e9185597d8",
   "metadata": {},
   "source": [
    "7. Функция для тюнинга гиперпараметров бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659da101-d65d-4d85-845a-b2cc7e331845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"loss\":[\"deviance\", \"exponential\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "def ohe_tr(onehotencoder, data, categorical_cols, not_fit):\n",
    "    if not_fit:\n",
    "        transformed_data = onehotencoder.fit_transform(data[categorical_cols])\n",
    "    else:\n",
    "        transformed_data = onehotencoder.transform(data[categorical_cols])\n",
    "    encoded_data = pd.DataFrame(transformed_data, index=data.index)\n",
    "    other_data = data.drop(columns=categorical_cols)\n",
    "    data_out = pd.concat([encoded_data, other_data], axis=1)\n",
    "    return data_out\n",
    "\n",
    "def train_test(n):\n",
    "    texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, trytexts,\n",
    "            text11, text12, text13, text14]\n",
    "    anss = [anss1, anss2, anss3, anss4, anss5, anss6, anss7, anss8, anss9, anss10, tryanswers,\n",
    "           anss11, anss12, anss13, anss14]\n",
    "\n",
    "    bigtext = ''\n",
    "    bigans = []\n",
    "\n",
    "    for text in texts[:n] + texts[n + 1:]:\n",
    "        bigtext += text\n",
    "\n",
    "    for ans in anss[:n] + anss[n + 1:]:\n",
    "        bigans += ans\n",
    "\n",
    "    table_train = make_table(bigtext, bigans)\n",
    "    table_test = make_table(texts[n], anss[n])\n",
    "\n",
    "    X_train = table_train.drop(['sign index', 'end?'], axis=1)\n",
    "    y_train = table_train['end?']\n",
    "    \n",
    "    categorical = ['sign', 'wright', 'right', 'left', 'abbleft', 'abbright', 'cright', \n",
    "                   'cleft', 'cwright', 'left_caps', 'right_caps']\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n",
    "   \n",
    "    X_train_trans = ohe_tr(enc, X_train, categorical, True)\n",
    "    X_test = table_test.drop(['sign index', 'end?'], axis=1)\n",
    "    y_test = table_test['end?']\n",
    "    X_test_trans = ohe_tr(enc, X_test, categorical, False)\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "            GradientBoostingClassifier(), parameters, scoring=f1, n_jobs= -1, cv = 3\n",
    "        )\n",
    "    clf.fit(X_train_trans, y_train)\n",
    "    clf.best_params_\n",
    "    \n",
    "    return clf.best_params_\n",
    "\n",
    "cnt = 14\n",
    "for k in range(0, cnt + 1):\n",
    "    print(k+1, '/', cnt + 1, train_test(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43018583-6cca-481d-940f-d5382729827f",
   "metadata": {},
   "source": [
    "8. Программа, разбивающая текст на предложения (итог работы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cb3ecbd-9671-4e98-827e-dd68053fd909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start():\n",
    "    # !pip install opencorpora-tools\n",
    "    # !pip install -U pymorphy2-dicts-ru\n",
    "\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from sklearn import svm\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.metrics import f1_score\n",
    "    import opencorpora\n",
    "    import os\n",
    "    import pickle\n",
    "    from re import match\n",
    "    import pymorphy2\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import f1_score, make_scorer\n",
    "    import numpy as np\n",
    "\n",
    "    with open(\"abbrevs\", \"rb\") as fp:\n",
    "        abr = pickle.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "    def find_abb(word, abbreviatures_list):\n",
    "        abbreviatures = dict()\n",
    "        for sent in abbreviatures_list:\n",
    "            srch = re.findall('(^|\\s)' + word + '[.]+[ ]+[А-Я]+[А-я]*', sent)\n",
    "            if srch:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def make_table(text):\n",
    "        possible_sentences = re.findall('[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*[.!?]', text)\n",
    "\n",
    "        pattern = '[.!?]'\n",
    "        possible_ends_list = []\n",
    "        possible_ends_indices = []\n",
    "\n",
    "        p = re.compile(pattern)\n",
    "\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        for m in p.finditer(text):\n",
    "            index = m.start()\n",
    "            character = m.group()\n",
    "            possible_ends_list.append(character)\n",
    "            possible_ends_indices.append(index)\n",
    "\n",
    "        left_pseudowords_list = []\n",
    "        right_pseudowords_list = []\n",
    "        left_pseudowords_count = []\n",
    "        right_pseudowords_count = []\n",
    "        right_abb_list = []\n",
    "        left_abb_list = []\n",
    "        cright_list = []\n",
    "        cleft_list = []\n",
    "        wright_list = []\n",
    "        cwright_list = []\n",
    "        # my\n",
    "        left_lengths = []\n",
    "        left_caps = []\n",
    "        right_caps = []\n",
    "\n",
    "        for i in range(len(possible_sentences)):\n",
    "            # load data\n",
    "            cur_sentence = possible_sentences[i]\n",
    "            if i + 1 != len(possible_sentences):\n",
    "                next_sentence = possible_sentences[i + 1]\n",
    "            else:\n",
    "                next_sentence = ''\n",
    "\n",
    "            # clean data\n",
    "            cur_sentence = re.sub(' +', ' ', cur_sentence).strip('( )*')    \n",
    "            next_sentence = re.sub(' +', ' ', next_sentence).strip('( )*')\n",
    "\n",
    "            # extracting pseudowords from data\n",
    "            left_pseudowords = re.split('[(\\n)|( )]', cur_sentence)\n",
    "            right_pseudowords = re.split('[(\\n)|( )]', next_sentence)\n",
    "            left_pseudoword = left_pseudowords[-1]\n",
    "            right_pseudoword = right_pseudowords[0]\n",
    "            left_abb = 0\n",
    "            right_abb = 0\n",
    "            cleft = ''\n",
    "            cright = ''\n",
    "            wright = list(filter(lambda v: match('[ЁёА-я0-9A-Za-z]+[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*', v), \n",
    "                                  right_pseudowords))        \n",
    "            cwright = ''\n",
    "\n",
    "            if len(wright) == 0:\n",
    "                wright = '---'\n",
    "            else:\n",
    "                wright = wright[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "\n",
    "            \"\"\"\n",
    "            testing\n",
    "            print(cur_sentence, left_pseudowords, left_pseudoword, len(left_pseudowords))\n",
    "            print(next_sentence, right_pseudowords, right_pseudoword, len(right_pseudowords))\n",
    "            print(len(possible_ends_list) == len(possible_sentences))\n",
    "            \"\"\"\n",
    "\n",
    "            if len(left_pseudoword) > 1:\n",
    "                left_pseudoword = left_pseudoword.strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "                left_abb = find_abb(left_pseudoword, abr)\n",
    "            elif left_pseudowords == ['']:\n",
    "                left_pseudowords = []\n",
    "            if len(left_pseudoword) == 0:\n",
    "                left_pseudowords = []\n",
    "                left_pseudoword = '---'\n",
    "\n",
    "            if len(right_pseudoword) > 1:\n",
    "                right_pseudoword = right_pseudowords[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "                right_abb = find_abb(right_pseudoword, abr)\n",
    "            elif right_pseudowords == ['']:\n",
    "                right_pseudowords = []\n",
    "            if len(right_pseudoword) == 0:\n",
    "                right_pseudowords = []\n",
    "                right_pseudoword = '---'\n",
    "\n",
    "            if wright != '---':\n",
    "                is_cyrillic = re.findall('[А-я]', wright)\n",
    "                is_latin = re.findall('[A-z]', wright)\n",
    "                is_numeric = re.findall('[0-9]', wright)\n",
    "                is_uppercase = re.findall('[А-Я]', wright[0])\n",
    "                is_lowercase = re.findall('[а-я]', wright[0])\n",
    "                is_first_num = re.findall('[0-9]', wright[0])\n",
    "                is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', wright[0])\n",
    "\n",
    "                if is_cyrillic:\n",
    "                    cwright += 'Cyrillic'\n",
    "                if is_latin:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Latin'\n",
    "                if is_numeric:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Numeric'\n",
    "                if is_uppercase:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Uppercase'\n",
    "                if is_lowercase:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Lowercase'\n",
    "\n",
    "                if is_first_num:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Numeric first'\n",
    "\n",
    "                if is_first_punct:\n",
    "                    if len(cwright) > 0:\n",
    "                        cwright += ' + '\n",
    "                    cwright += 'Punctuation first'\n",
    "\n",
    "                if len(cwright) == 0:\n",
    "                    cwright = '---'\n",
    "            else:\n",
    "                cwright = '---'\n",
    "\n",
    "            if len(right_pseudoword) > 0:\n",
    "                is_cyrillic = re.findall('[А-я]', right_pseudoword)\n",
    "                is_latin = re.findall('[A-z]', right_pseudoword)\n",
    "                is_numeric = re.findall('[0-9]', right_pseudoword)\n",
    "                is_uppercase = re.findall('[А-Я]', right_pseudoword[0])\n",
    "                is_lowercase = re.findall('[а-я]', right_pseudoword[0])\n",
    "                is_first_num = re.findall('[0-9]', right_pseudoword[0])\n",
    "                is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', right_pseudoword[0])\n",
    "\n",
    "                if is_cyrillic:\n",
    "                    cright += 'Cyrillic'\n",
    "                if is_latin:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Latin'\n",
    "                if is_numeric:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Numeric'\n",
    "                if is_uppercase:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Uppercase'\n",
    "                if is_lowercase:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Lowercase'\n",
    "\n",
    "                if is_first_num:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Numeric first'\n",
    "\n",
    "                if is_first_punct:\n",
    "                    if len(cright) > 0:\n",
    "                        cright += ' + '\n",
    "                    cright += 'Punctuation first'\n",
    "\n",
    "                if len(cright) == 0:\n",
    "                    cright = '---'\n",
    "\n",
    "            else:\n",
    "                cright = '---'\n",
    "\n",
    "            if len(left_pseudoword) > 0:\n",
    "                is_cyrillic = re.findall('[А-я]', left_pseudoword)\n",
    "                is_latin = re.findall('[A-z]', left_pseudoword)\n",
    "                is_numeric = re.findall('[0-9]', left_pseudoword)\n",
    "                is_uppercase = re.findall('[А-Я]', left_pseudoword[0])\n",
    "                is_lowercase = re.findall('[а-я]', left_pseudoword[0])\n",
    "                is_first_num = re.findall('[0-9]', left_pseudoword[0])\n",
    "                is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', left_pseudoword[0])\n",
    "\n",
    "                if is_cyrillic:\n",
    "                    cleft += 'Cyrillic'\n",
    "                if is_latin:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Latin'\n",
    "                if is_numeric:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Numeric'\n",
    "                if is_uppercase:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Uppercase'\n",
    "                if is_lowercase:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Lowercase'\n",
    "\n",
    "                if is_first_num:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Numeric first'\n",
    "\n",
    "                if is_first_punct:\n",
    "                    if len(cleft) > 0:\n",
    "                        cleft += ' + '\n",
    "                    cleft += 'Punctuation first'\n",
    "\n",
    "                if len(cleft) == 0:\n",
    "                    cleft = '---'\n",
    "\n",
    "            else:\n",
    "                cleft = '---'\n",
    "\n",
    "            # saving pseudowords extracted data\n",
    "            left_pseudowords_list.append(morph.parse(left_pseudoword)[0].normal_form)\n",
    "            right_pseudowords_list.append(morph.parse(right_pseudoword)[0].normal_form)\n",
    "            left_pseudowords_count.append(len(left_pseudowords))\n",
    "            right_pseudowords_count.append(len(right_pseudowords))\n",
    "            left_abb_list.append(left_abb)\n",
    "            right_abb_list.append(right_abb)\n",
    "            cleft_list.append(cleft)\n",
    "            cright_list.append(cright)\n",
    "            wright_list.append(morph.parse(wright)[0].normal_form)\n",
    "            cwright_list.append(cwright)\n",
    "            # my\n",
    "            left_lengths.append(len(left_pseudoword))\n",
    "            left_caps.append(len(re.findall('[А-Я]', left_pseudoword)) == len(left_pseudoword))\n",
    "            right_caps.append(len(re.findall('[А-Я]', right_pseudoword)) == len(right_pseudoword))\n",
    "\n",
    "        data_for_df = dict()\n",
    "\n",
    "        data_for_df['sign'] = possible_ends_list\n",
    "        data_for_df['sign index'] = possible_ends_indices\n",
    "        data_for_df['right'] = right_pseudowords_list\n",
    "        data_for_df['left'] = left_pseudowords_list\n",
    "        data_for_df['dleft'] = left_pseudowords_count\n",
    "        data_for_df['dright'] = right_pseudowords_count\n",
    "        data_for_df['abbleft'] = left_abb_list\n",
    "        data_for_df['abbright'] = right_abb_list\n",
    "        data_for_df['cright'] = cright_list\n",
    "        data_for_df['cleft'] = cleft_list\n",
    "        data_for_df['wright'] = wright_list\n",
    "        data_for_df['cwright'] = cwright_list\n",
    "        data_for_df['left_caps'] = left_caps\n",
    "        data_for_df['right_caps'] = right_caps\n",
    "\n",
    "        possible_ends_df = pd.DataFrame(data_for_df)\n",
    "\n",
    "        return possible_ends_df\n",
    "\n",
    "    def ohe_tr(onehotencoder, data, categorical_cols, not_fit):\n",
    "        if not_fit:\n",
    "            transformed_data = onehotencoder.fit_transform(data[categorical_cols])\n",
    "        else:\n",
    "            transformed_data = onehotencoder.transform(data[categorical_cols])\n",
    "        encoded_data = pd.DataFrame(transformed_data, index=data.index)\n",
    "        other_data = data.drop(columns=categorical_cols)\n",
    "        data_out = pd.concat([encoded_data, other_data], axis=1)\n",
    "        return data_out\n",
    "\n",
    "    with open('model.pkl', 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print('Введите название текстового файла с постановлением ФАС Уральского округа')\n",
    "\n",
    "    filename = input()\n",
    "\n",
    "    with open(filename, encoding='cp1251', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    with open('enc.pkl', 'rb') as fp:\n",
    "        enc = pickle.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "    table_test = make_table(text)\n",
    "\n",
    "    X_test = table_test.drop(['sign index'], axis=1)\n",
    "\n",
    "    X_test_trans = ohe_tr(enc, X_test, categorical, False)\n",
    "    y_pred = clf.predict(X_test_trans)\n",
    "\n",
    "    answer_sentences = re.findall('[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z\\n\\t]*[.!?]', text)\n",
    "    answer = ''\n",
    "    for i in range(len(answer_sentences)):\n",
    "        if y_pred[i] == 1:\n",
    "            answer += answer_sentences[i].strip('( )*') + '\\n'\n",
    "        else:\n",
    "            answer += answer_sentences[i].strip('( )*')\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06aa2dc-079a-4d84-aacd-f50e2ba4b661",
   "metadata": {},
   "source": [
    "9. Функция для тестирования прочих моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d96df036-3e79-4a86-88f1-ec0bb389eb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def find_abb(word, abbreviatures_list):\n",
    "    abbreviatures = dict()\n",
    "    for sent in abbreviatures_list:\n",
    "        srch = re.findall('(^|\\s)' + word + '[.]+[ ]+[А-Я]+[А-я]*', sent)\n",
    "        if srch:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def make_table(text, answers):\n",
    "    possible_sentences = re.findall('[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*[.!?]', text)\n",
    "\n",
    "    pattern = '[.!?]'\n",
    "    possible_ends_list = []\n",
    "    possible_ends_indices = []\n",
    "\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    for m in p.finditer(text):\n",
    "        index = m.start()\n",
    "        character = m.group()\n",
    "        possible_ends_list.append(character)\n",
    "        possible_ends_indices.append(index)\n",
    "\n",
    "    left_pseudowords_list = []\n",
    "    right_pseudowords_list = []\n",
    "    left_pseudowords_count = []\n",
    "    right_pseudowords_count = []\n",
    "    right_abb_list = []\n",
    "    left_abb_list = []\n",
    "    cright_list = []\n",
    "    cleft_list = []\n",
    "    wright_list = []\n",
    "    cwright_list = []\n",
    "    left_lengths = []\n",
    "    left_caps = []\n",
    "    right_caps = []\n",
    "\n",
    "    for i in range(len(possible_sentences)):\n",
    "        # load data\n",
    "        cur_sentence = possible_sentences[i]\n",
    "        if i + 1 != len(possible_sentences):\n",
    "            next_sentence = possible_sentences[i + 1]\n",
    "        else:\n",
    "            next_sentence = ''\n",
    "            \n",
    "        # clean data\n",
    "        cur_sentence = re.sub(' +', ' ', cur_sentence).strip('( )*')    \n",
    "        next_sentence = re.sub(' +', ' ', next_sentence).strip('( )*')\n",
    "\n",
    "        # extracting pseudowords from data\n",
    "        left_pseudowords = re.split('[(\\n)|( )]', cur_sentence)\n",
    "        right_pseudowords = re.split('[(\\n)|( )]', next_sentence)\n",
    "        left_pseudoword = left_pseudowords[-1]\n",
    "        right_pseudoword = right_pseudowords[0]\n",
    "        left_abb = 0\n",
    "        right_abb = 0\n",
    "        cleft = ''\n",
    "        cright = ''\n",
    "        wright = list(filter(lambda v: match('[ЁёА-я0-9A-Za-z]+[ЁёА-я0-9 ,()+\\-/\"@#№$%_=*;&^<>—:A-Za-z]*', v), \n",
    "                                  right_pseudowords))        \n",
    "        cwright = ''\n",
    "\n",
    "        if len(wright) == 0:\n",
    "            wright = '---'\n",
    "        else:\n",
    "            wright = wright[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "\n",
    "            \"\"\"\n",
    "            testing\n",
    "            print(cur_sentence, left_pseudowords, left_pseudoword, len(left_pseudowords))\n",
    "            print(next_sentence, right_pseudowords, right_pseudoword, len(right_pseudowords))\n",
    "            print(len(possible_ends_list) == len(possible_sentences))\n",
    "            \"\"\"\n",
    "\n",
    "        if len(left_pseudoword) > 1:\n",
    "            left_pseudoword = left_pseudoword.strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "            left_abb = find_abb(left_pseudoword, abr)\n",
    "        elif left_pseudowords == ['']:\n",
    "            left_pseudowords = []\n",
    "        if len(left_pseudoword) == 0:\n",
    "            left_pseudowords = []\n",
    "            left_pseudoword = '---'\n",
    "\n",
    "        if len(right_pseudoword) > 1:\n",
    "            right_pseudoword = right_pseudowords[0].strip(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]*')\n",
    "            right_abb = find_abb(right_pseudoword, abr)\n",
    "        elif right_pseudowords == ['']:\n",
    "            right_pseudowords = []\n",
    "        if len(right_pseudoword) == 0:\n",
    "            right_pseudowords = []\n",
    "            right_pseudoword = '---'\n",
    "\n",
    "        if wright != '---':\n",
    "            is_cyrillic = re.findall('[А-я]', wright)\n",
    "            is_latin = re.findall('[A-z]', wright)\n",
    "            is_numeric = re.findall('[0-9]', wright)\n",
    "            is_uppercase = re.findall('[А-Я]', wright[0])\n",
    "            is_lowercase = re.findall('[а-я]', wright[0])\n",
    "            is_first_num = re.findall('[0-9]', wright[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', wright[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cwright += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cwright) > 0:\n",
    "                    cwright += ' + '\n",
    "                cwright += 'Punctuation first'\n",
    "\n",
    "            if len(cwright) == 0:\n",
    "                cwright = '---'\n",
    "        else:\n",
    "            cwright = '---'\n",
    "\n",
    "        if len(right_pseudoword) > 0:\n",
    "            is_cyrillic = re.findall('[А-я]', right_pseudoword)\n",
    "            is_latin = re.findall('[A-z]', right_pseudoword)\n",
    "            is_numeric = re.findall('[0-9]', right_pseudoword)\n",
    "            is_uppercase = re.findall('[А-Я]', right_pseudoword[0])\n",
    "            is_lowercase = re.findall('[а-я]', right_pseudoword[0])\n",
    "            is_first_num = re.findall('[0-9]', right_pseudoword[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', right_pseudoword[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cright += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cright) > 0:\n",
    "                    cright += ' + '\n",
    "                cright += 'Punctuation first'\n",
    "\n",
    "            if len(cright) == 0:\n",
    "                cright = '---'\n",
    "\n",
    "        else:\n",
    "            cright = '---'\n",
    "\n",
    "        if len(left_pseudoword) > 0:\n",
    "            is_cyrillic = re.findall('[А-я]', left_pseudoword)\n",
    "            is_latin = re.findall('[A-z]', left_pseudoword)\n",
    "            is_numeric = re.findall('[0-9]', left_pseudoword)\n",
    "            is_uppercase = re.findall('[А-Я]', left_pseudoword[0])\n",
    "            is_lowercase = re.findall('[а-я]', left_pseudoword[0])\n",
    "            is_first_num = re.findall('[0-9]', left_pseudoword[0])\n",
    "            is_first_punct = re.findall(',()+\\-/\"@#№$%_=*;&^<>—:A-Za-z.!?]', left_pseudoword[0])\n",
    "\n",
    "            if is_cyrillic:\n",
    "                cleft += 'Cyrillic'\n",
    "            if is_latin:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Latin'\n",
    "            if is_numeric:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Numeric'\n",
    "            if is_uppercase:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Uppercase'\n",
    "            if is_lowercase:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Lowercase'\n",
    "\n",
    "            if is_first_num:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Numeric first'\n",
    "\n",
    "            if is_first_punct:\n",
    "                if len(cleft) > 0:\n",
    "                    cleft += ' + '\n",
    "                cleft += 'Punctuation first'\n",
    "\n",
    "            if len(cleft) == 0:\n",
    "                cleft = '---'\n",
    "\n",
    "        else:\n",
    "            cleft = '---'\n",
    "\n",
    "            # saving pseudowords extracted data\n",
    "        left_pseudowords_list.append(morph.parse(left_pseudoword)[0].normal_form)\n",
    "        right_pseudowords_list.append(morph.parse(right_pseudoword)[0].normal_form)\n",
    "        left_pseudowords_count.append(len(left_pseudowords))\n",
    "        right_pseudowords_count.append(len(right_pseudowords))\n",
    "        left_abb_list.append(left_abb)\n",
    "        right_abb_list.append(right_abb)\n",
    "        cleft_list.append(cleft)\n",
    "        cright_list.append(cright)\n",
    "        wright_list.append(morph.parse(wright)[0].normal_form)\n",
    "        cwright_list.append(cwright)\n",
    "        # my\n",
    "        left_lengths.append(len(left_pseudoword))\n",
    "        left_caps.append(len(re.findall('[А-Я]', left_pseudoword)) == len(left_pseudoword))\n",
    "        right_caps.append(len(re.findall('[А-Я]', right_pseudoword)) == len(right_pseudoword))\n",
    "\n",
    "        data_for_df = dict()\n",
    "\n",
    "        data_for_df['sign'] = possible_ends_list\n",
    "        data_for_df['sign index'] = possible_ends_indices\n",
    "        data_for_df['right'] = right_pseudowords_list\n",
    "        data_for_df['left'] = left_pseudowords_list\n",
    "        data_for_df['dleft'] = left_pseudowords_count\n",
    "        data_for_df['dright'] = right_pseudowords_count\n",
    "        data_for_df['abbleft'] = left_abb_list\n",
    "        data_for_df['abbright'] = right_abb_list\n",
    "        data_for_df['cright'] = cright_list\n",
    "        data_for_df['cleft'] = cleft_list\n",
    "        data_for_df['wright'] = wright_list\n",
    "        data_for_df['cwright'] = cwright_list\n",
    "        data_for_df['left_caps'] = left_caps\n",
    "        data_for_df['right_caps'] = right_caps\n",
    "        data_for_df['end?'] = answers\n",
    "\n",
    "    possible_ends_df = pd.DataFrame(data_for_df)\n",
    "    \n",
    "    return possible_ends_df\n",
    "\n",
    "def test_other():\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-3415_rzm', \"rb\") as fp:\n",
    "        anss1 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-3415.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text1 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee1 = make_table(text1, anss1)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-2600_rzm', \"rb\") as fp:\n",
    "        anss2 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-2600.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text2 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee2 = make_table(text2, anss2)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4528_rzm', \"rb\") as fp:\n",
    "        anss3 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4528.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text3 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee3 = make_table(text3, anss3)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1066_rzm', \"rb\") as fp:\n",
    "        anss4 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1066.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text4 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee4 = make_table(text4, anss4)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4409_rzm', \"rb\") as fp:\n",
    "        anss5 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4409.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text5 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee5 = make_table(text5, anss5)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 15_12_2011 N Ф09-8320_rzm', \"rb\") as fp:\n",
    "        anss6 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 15_12_2011 N Ф09-8320.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text6 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee6 = make_table(text6, anss6)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4438_rzm', \"rb\") as fp:\n",
    "        anss7 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4438.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text7 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee7 = make_table(text7, anss7)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 29_01_2002 N Ф09-92 0_rzm', \"rb\") as fp:\n",
    "        anss8 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 29_01_2002 N Ф09-92 0.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text8 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee8 = make_table(text8, anss8)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 27_07_2012 N Ф09-6215_rzm', \"rb\") as fp:\n",
    "        anss9 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 27_07_2012 N Ф09-6215.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text9 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee9 = make_table(text9, anss9)\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4435_rzm', \"rb\") as fp:\n",
    "        anss10 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4435.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text10 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee10 = make_table(text10, anss10); плохо сделал именно этот текст\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4559_rzm', \"rb\") as fp:\n",
    "        anss11 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 31_05_2006 N Ф09-4559.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text11 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee11\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-9906_rzm', \"rb\") as fp:\n",
    "        anss12 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-9906.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text12 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee12\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4336_rzm', \"rb\") as fp:\n",
    "        anss13 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4336.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text13 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee13\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 05_06_2006 N Ф09-205_rzm', \"rb\") as fp:\n",
    "        anss14 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 05_06_2006 N Ф09-205.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text14 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee14\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4131_rzm', \"rb\") as fp:\n",
    "        anss15 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_05_2006 N Ф09-4131.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text15 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee15\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 06_02_2007 N Ф09-4881_rzm', \"rb\") as fp:\n",
    "        anss16 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 06_02_2007 N Ф09-4881.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text16 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee16\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_07_2012 N Ф09-1027_rzm', \"rb\") as fp:\n",
    "        anss17 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 30_07_2012 N Ф09-1027.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text17 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee17\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4469_rzm', \"rb\") as fp:\n",
    "        anss18 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4469.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text18 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee18\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1060_rzm', \"rb\") as fp:\n",
    "        anss19 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 20_12_2010 N Ф09-1060.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text19 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee19\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4398_rzm', \"rb\") as fp:\n",
    "        anss20 = pickle.load(fp)\n",
    "        # print(anss)\n",
    "    fp.close()\n",
    "\n",
    "    with open('raw_text/Постановление ФАС Уральского округа от 01_06_2006 N Ф09-4398.txt',\n",
    "             encoding='cp1251', errors='ignore') as fp:\n",
    "        text20 = fp.read()\n",
    "        # print(text)\n",
    "    fp.close()\n",
    "\n",
    "    # tablee20\n",
    "\n",
    "    def ohe_tr(onehotencoder, data, categorical_cols, not_fit):\n",
    "        if not_fit:\n",
    "            transformed_data = onehotencoder.fit_transform(data[categorical_cols])\n",
    "        else:\n",
    "            transformed_data = onehotencoder.transform(data[categorical_cols])\n",
    "        encoded_data = pd.DataFrame(transformed_data, index=data.index)\n",
    "        other_data = data.drop(columns=categorical_cols)\n",
    "        data_out = pd.concat([encoded_data, other_data], axis=1)\n",
    "        return data_out\n",
    "\n",
    "    def train_test(n, clf):\n",
    "        texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10,\n",
    "                text11, text12, text13, text14, text15, text16, text17, text18, text19,\n",
    "                text20]\n",
    "        anss = [anss1, anss2, anss3, anss4, anss5, anss6, anss7, anss8, anss9, anss10,\n",
    "               anss11, anss12, anss13, anss14, anss15, anss16, anss17, anss18, anss19,\n",
    "               anss20]\n",
    "\n",
    "        bigtext = ''\n",
    "        bigans = []\n",
    "\n",
    "        for text in texts[:n] + texts[n + 1:]:\n",
    "            bigtext += text\n",
    "\n",
    "        for ans in anss[:n] + anss[n + 1:]:\n",
    "            bigans += ans\n",
    "\n",
    "        table_train = make_table(bigtext, bigans)\n",
    "        table_test = make_table(texts[n], anss[n])\n",
    "\n",
    "        X_train = table_train.drop(['sign index', 'end?'], axis=1)\n",
    "        y_train = table_train['end?']\n",
    "\n",
    "        categorical = ['sign', 'wright', 'right', 'left', 'abbleft', 'abbright', 'cright', 'cleft', 'cwright', 'left_caps', 'right_caps']\n",
    "\n",
    "        X_train_trans = ohe_tr(enc, X_train, categorical, True)\n",
    "\n",
    "        clf.fit(X_train_trans, y_train)\n",
    "\n",
    "        X_test = table_test.drop(['sign index', 'end?'], axis=1)\n",
    "        y_test = table_test['end?']\n",
    "\n",
    "        X_test_trans = ohe_tr(enc, X_test, categorical, False)\n",
    "        y_pred = clf.predict(X_test_trans)\n",
    "        \n",
    "        return f1_score(y_test, y_pred)\n",
    "\n",
    "    cnt = 20\n",
    "    clf1 = DecisionTreeClassifier()\n",
    "    clf2 = LogisticRegression()\n",
    "    clf3 = RandomForestClassifier()\n",
    "    clf4 = GradientBoostingClassifier()\n",
    "    clf5 = SVC()\n",
    "    for alg in [clf1, clf2, clf3, clf4, clf4]:\n",
    "        if alg == clf1:\n",
    "            print('DecisionTreeClassifier')\n",
    "        elif alg == clf2:\n",
    "            print('LogisticRegression')\n",
    "        elif alg == clf3:\n",
    "            print('RandomForestClassifier')\n",
    "        elif alg == clf4:\n",
    "            print('GradientBoostingClassifier')\n",
    "        else:\n",
    "            print('SVC')\n",
    "        for k in range(0, cnt):\n",
    "            print(k + 1, '/', cnt, train_test(k, clf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
